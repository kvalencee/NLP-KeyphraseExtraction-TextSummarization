{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"89aad950-d833-496f-8d65-acf997376724\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Cabecera\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Nombre completo del estudiante**: [TU NOMBRE AQUI]\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Grupo**: [TU GRUPO]  \\n\",\n",
    "    \"\\n\",\n",
    "    \"**Carrera**: Ingeniería en Inteligencia Artificial  \\n\",\n",
    "    \"\\n\",\n",
    "    \"**Fecha de última modificación**: 04/05/2025\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Descripción detallada del programa\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este programa implementa diferentes algoritmos de extracción de frases clave y resumen automático de textos aplicados a las tres primeras cartas del libro \\\"Frankenstein\\\" de Mary Shelley. Los algoritmos utilizados son:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. TF-IDF (Term Frequency-Inverse Document Frequency)\\n\",\n",
    "    \"2. Frecuencia de palabras normalizada\\n\",\n",
    "    \"3. RAKE (Rapid Automatic Keyword Extraction)\\n\",\n",
    "    \"4. TextRank\\n\",\n",
    "    \"5. BERT (Bidirectional Encoder Representations from Transformers)\\n\",\n",
    "    \"6. LSA (Latent Semantic Analysis)\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Datos de entrada:\\n\",\n",
    "    \"- **Texto de entrada**: Las tres primeras cartas del libro \\\"Frankenstein\\\" descargado de Project Gutenberg.\\n\",\n",
    "    \"- **Algoritmos de procesamiento**: Módulos de NLTK, SpaCy, Transformers y otras bibliotecas especializadas.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Salidas esperadas:\\n\",\n",
    "    \"- Resúmenes extractivos de cada carta usando cada algoritmo\\n\",\n",
    "    \"- Métricas de tiempo de ejecución\\n\",\n",
    "    \"- Análisis comparativo de resultados\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"## PRÁCTICA 3: IDENTIFICACIÓN DE FRASES CLAVE Y RESUMEN AUTOMÁTICO DE TEXTO\\n\",\n",
    "    \"\\n\",\n",
    "    \"### 1. Importación de bibliotecas\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b1eac2da-4402-4f69-bf7d-c530b66e56c0\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Bibliotecas principales\\n\",\n",
    "    \"import requests\\n\",\n",
    "    \"import re\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.tokenize import sent_tokenize, word_tokenize\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.stem import PorterStemmer, WordNetLemmatizer\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Algoritmos específicos\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"import spacy\\n\",\n",
    "    \"from rake_nltk import Rake\\n\",\n",
    "    \"import networkx as nx\\n\",\n",
    "    \"from transformers import pipeline\\n\",\n",
    "    \"from sklearn.decomposition import TruncatedSVD\\n\",\n",
    "    \"from sklearn.feature_extraction.text import CountVectorizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Descarga de recursos NLTK necesarios\\n\",\n",
    "    \"nltk.download('punkt')\\n\",\n",
    "    \"nltk.download('stopwords')\\n\",\n",
    "    \"nltk.download('wordnet')\\n\",\n",
    "    \"nltk.download('averaged_perceptron_tagger')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Cargar SpaCy\\n\",\n",
    "    \"nlp = spacy.load('en_core_web_sm')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"7d9ea830-5df5-4b78-9342-58aed1857961\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 2. Generación del corpus: Descarga y extracción de las tres primeras cartas de Frankenstein\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"51708a64-201e-440a-ad52-7bad5941e5ab\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Descargar el libro Frankenstein\\n\",\n",
    "    \"url = \\\"https://www.gutenberg.org/files/84/84-0.txt\\\"\\n\",\n",
    "    \"response = requests.get(url)\\n\",\n",
    "    \"text = response.text\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Encontrar el inicio y fin de las cartas\\n\",\n",
    "    \"carta_start = text.find(\\\"LETTER I\\\")\\n\",\n",
    "    \"carta_end = text.find(\\\"CHAPTER 1\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extraer la sección de cartas\\n\",\n",
    "    \"cartas_text = text[carta_start:carta_end]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Separar las tres primeras cartas\\n\",\n",
    "    \"cartas = []\\n\",\n",
    "    \"carta_indices = [\\\"LETTER I\\\", \\\"LETTER II\\\", \\\"LETTER III\\\"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i in range(len(carta_indices)):\\n\",\n",
    "    \"    start_idx = cartas_text.find(carta_indices[i])\\n\",\n",
    "    \"    if i + 1 < len(carta_indices):\\n\",\n",
    "    \"        end_idx = cartas_text.find(carta_indices[i + 1])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        end_idx = cartas_text.find(\\\"LETTER IV\\\", start_idx)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    carta = cartas_text[start_idx:end_idx]\\n\",\n",
    "    \"    cartas.append(carta.strip())\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Cartas extraídas: {len(cartas)}\\\")\\n\",\n",
    "    \"print(f\\\"Longitud carta 1: {len(cartas[0])} caracteres\\\")\\n\",\n",
    "    \"print(f\\\"Longitud carta 2: {len(cartas[1])} caracteres\\\")\\n\",\n",
    "    \"print(f\\\"Longitud carta 3: {len(cartas[2])} caracteres\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Mostrar el inicio de cada carta para verificación\\n\",\n",
    "    \"for i, carta in enumerate(cartas):\\n\",\n",
    "    \"    print(f\\\"\\\\n=== CARTA {i+1} - Primeras 200 caracteres ===\\\\n\\\")\\n\",\n",
    "    \"    print(carta[:200])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"53a821e2-e2ec-4491-bc29-4f9492a351ac\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 3. Normalización de textos\\n\",\n",
    "    \"\\n\",\n",
    "    \"Para cada documento, aplicaremos las siguientes técnicas de normalización:\\n\",\n",
    "    \"1. Eliminación de encabezados y formato de Project Gutenberg\\n\",\n",
    "    \"2. Limpieza de caracteres especiales\\n\",\n",
    "    \"3. Tokenización\\n\",\n",
    "    \"4. Eliminación de palabras vacías\\n\",\n",
    "    \"5. Lematización/Stemming\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"a0bf73af-3a78-4790-b378-675ca408e298\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def normalize_text(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"Normaliza el texto aplicando varias técnicas de preprocesamiento\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 1. Eliminar encabezados y metadatos de Project Gutenberg\\n\",\n",
    "    \"    text = re.sub(r'^.*?LETTER [I]+', '', text)\\n\",\n",
    "    \"    text = re.sub(r'_.*?_', ' ', text)  # eliminar texto entre guiones bajos\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 2. Limpiar caracteres especiales y múltiples espacios\\n\",\n",
    "    \"    text = re.sub(r'[^a-zA-Z0-9\\\\s.,!?]', ' ', text)\\n\",\n",
    "    \"    text = re.sub(r'\\\\s+', ' ', text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 3. Convertir a minúsculas\\n\",\n",
    "    \"    text = text.lower()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 4. Tokenización por oraciones\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 5. Tokenización por palabras y eliminación de palabras vacías\\n\",\n",
    "    \"    stop_words = set(stopwords.words('english'))\\n\",\n",
    "    \"    lemmatizer = WordNetLemmatizer()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    processed_sentences = []\\n\",\n",
    "    \"    for sentence in sentences:\\n\",\n",
    "    \"        words = word_tokenize(sentence)\\n\",\n",
    "    \"        words = [lemmatizer.lemmatize(word) for word in words \\n\",\n",
    "    \"                if word.isalpha() and word not in stop_words]\\n\",\n",
    "    \"        processed_sentences.append(' '.join(words))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return ' '.join(processed_sentences)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Normalizar las cartas\\n\",\n",
    "    \"cartas_normalizadas = []\\n\",\n",
    "    \"cartas_originals = []  # Guardamos las originales para el resumen\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, carta in enumerate(cartas):\\n\",\n",
    "    \"    # Guardar versión original para resúmenes\\n\",\n",
    "    \"    carta_clean = re.sub(r'^.*?LETTER [I]+', '', carta)\\n\",\n",
    "    \"    carta_clean = re.sub(r'_.*?_', ' ', carta_clean)\\n\",\n",
    "    \"    cartas_originals.append(carta_clean.strip())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Versión normalizada para procesamiento\\n\",\n",
    "    \"    normalized = normalize_text(carta)\\n\",\n",
    "    \"    cartas_normalizadas.append(normalized)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n=== CARTA {i+1} - Texto normalizado (primeras 200 caracteres) ===\\\")\\n\",\n",
    "    \"    print(normalized[:200])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"d45f066a-d960-479a-949b-a348e63920e7\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### 4. Resumen automático extractivo de texto\\n\",\n",
    "    \"\\n\",\n",
    "    \"#### 4.1 TF-IDF con NLTK\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"575be191-1338-4fad-b196-2118606fbc08\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def tfidf_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando TF-IDF\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Vectorizar usando TF-IDF\\n\",\n",
    "    \"    tfidf_vectorizer = TfidfVectorizer()\\n\",\n",
    "    \"    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular la suma de TF-IDF para cada oración\\n\",\n",
    "    \"    sentence_scores = tfidf_matrix.sum(axis=1).A1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones mejor puntuadas\\n\",\n",
    "    \"    top_indices = sentence_scores.argsort()[-num_sentences:][::-1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"tfidf_times = []\\n\",\n",
    "    \"tfidf_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO TF-IDF ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = tfidf_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    tfidf_times.append(execution_time)\\n\",\n",
    "    \"    tfidf_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"93526356-aef6-4353-866e-28656afbac85\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### 4.2 Frecuencia de palabras normalizada\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"c5c42cab-b5d0-40b1-ab2e-629ef13a53eb\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def word_frequency_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando frecuencia de palabras normalizada\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenización\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    words = word_tokenize(text.lower())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Eliminar palabras vacías\\n\",\n",
    "    \"    stop_words = set(stopwords.words('english'))\\n\",\n",
    "    \"    words = [word for word in words if word.isalpha() and word not in stop_words]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular frecuencia de palabras\\n\",\n",
    "    \"    word_freq = {}\\n\",\n",
    "    \"    for word in words:\\n\",\n",
    "    \"        if word in word_freq:\\n\",\n",
    "    \"            word_freq[word] += 1\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            word_freq[word] = 1\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Normalizar frecuencias\\n\",\n",
    "    \"    max_freq = max(word_freq.values())\\n\",\n",
    "    \"    for word in word_freq:\\n\",\n",
    "    \"        word_freq[word] = word_freq[word] / max_freq\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Puntuar oraciones\\n\",\n",
    "    \"    sentence_scores = []\\n\",\n",
    "    \"    for sentence in sentences:\\n\",\n",
    "    \"        sentence_words = word_tokenize(sentence.lower())\\n\",\n",
    "    \"        score = 0\\n\",\n",
    "    \"        word_count = 0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for word in sentence_words:\\n\",\n",
    "    \"            if word in word_freq:\\n\",\n",
    "    \"                score += word_freq[word]\\n\",\n",
    "    \"                word_count += 1\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if word_count > 0:\\n\",\n",
    "    \"            sentence_scores.append(score / word_count)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            sentence_scores.append(0)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones mejor puntuadas\\n\",\n",
    "    \"    top_indices = sorted(range(len(sentence_scores)), \\n\",\n",
    "    \"                        key=lambda i: sentence_scores[i], \\n\",\n",
    "    \"                        reverse=True)[:num_sentences]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"freq_times = []\\n\",\n",
    "    \"freq_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO FRECUENCIA DE PALABRAS NORMALIZADA ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = word_frequency_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    freq_times.append(execution_time)\\n\",\n",
    "    \"    freq_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"80d8a7f4-f551-4e73-9033-e473b1881341\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### 4.3 RAKE (Rapid Automatic Keyword Extraction)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"3dcb4dc2-7d51-4cf8-8b26-ebc62c32c316\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def rake_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando RAKE para extraer frases clave\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Inicializar RAKE\\n\",\n",
    "    \"    r = Rake()\\n\",\n",
    "    \"    r.extract_keywords_from_text(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener frases clave con puntuación\\n\",\n",
    "    \"    phrases = r.get_ranked_phrases_with_scores()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener oraciones del texto\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Puntuar oraciones basándose en la presencia de frases clave\\n\",\n",
    "    \"    sentence_scores = []\\n\",\n",
    "    \"    for sentence in sentences:\\n\",\n",
    "    \"        score = 0\\n\",\n",
    "    \"        for phrase_score, phrase in phrases:\\n\",\n",
    "    \"            if phrase.lower() in sentence.lower():\\n\",\n",
    "    \"                score += phrase_score\\n\",\n",
    "    \"        sentence_scores.append(score)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones mejor puntuadas\\n\",\n",
    "    \"    top_indices = sorted(range(len(sentence_scores)), \\n\",\n",
    "    \"                        key=lambda i: sentence_scores[i], \\n\",\n",
    "    \"                        reverse=True)[:num_sentences]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"rake_times = []\\n\",\n",
    "    \"rake_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO RAKE ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = rake_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    rake_times.append(execution_time)\\n\",\n",
    "    \"    rake_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"c5dfb249-78f3-4e5c-aae1-4f416dc5c71e\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### 4.4 TextRank\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"46c25d35-c7bc-4a83-9008-8539ca48d4d7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def textrank_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando TextRank\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenizar oraciones\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Crear matriz de similitud\\n\",\n",
    "    \"    vectorizer = TfidfVectorizer()\\n\",\n",
    "    \"    tfidf_matrix = vectorizer.fit_transform(sentences)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular similitud usando multiplicación de matrices\\n\",\n",
    "    \"    similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Crear grafo usando NetworkX\\n\",\n",
    "    \"    graph = nx.from_numpy_array(similarity_matrix)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Aplicar SVD (Singular Value Decomposition)\\n\",\n",
    "    \"    svd = TruncatedSVD(n_components=4)  # Reducir a 4 dimensiones\\n\",\n",
    "    \"    document_matrix_reduced = svd.fit_transform(document_matrix)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular puntuación para cada oración basada en los componentes principales\\n\",\n",
    "    \"    sentence_scores = []\\n\",\n",
    "    \"    for i in range(len(sentences)):\\n\",\n",
    "    \"        # Sumar el valor absoluto de cada componente\\n\",\n",
    "    \"        score = sum(abs(document_matrix_reduced[i]))\\n\",\n",
    "    \"        sentence_scores.append(score)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones mejor puntuadas\\n\",\n",
    "    \"    top_indices = sorted(range(len(sentence_scores)), \\n\",\n",
    "    \"                        key=lambda i: sentence_scores[i], \\n\",\n",
    "    \"                        reverse=True)[:num_sentences]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"lsa_times = []\\n\",\n",
    "    \"lsa_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO LSA ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = lsa_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    lsa_times.append(execution_time)\\n\",\n",
    "    \"    lsa_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\") PageRank (TextRank)\\n\",\n",
    "    \"    scores = nx.pagerank(graph)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones mejor puntuadas\\n\",\n",
    "    \"    top_indices = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"textrank_times = []\\n\",\n",
    "    \"textrank_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO TEXTRANK ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = textrank_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    textrank_times.append(execution_time)\\n\",\n",
    "    \"    textrank_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"08dd7914-4f33-4e38-b41e-10b2653e249c\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### 4.5 BERT con Transformers\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"aed33907-4d58-4bbd-96aa-dac964670ad7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def bert_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando BERT para embeddings de oraciones\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenizar oraciones\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Usar BERT para generar embeddings\\n\",\n",
    "    \"    # Nota: En ambiente real, usaríamos un modelo específico para embeddings\\n\",\n",
    "    \"    # Aquí usaremos un approach más simple con BERT pre-entrenado\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Vectorizar usando SpaCy (como aproximación a BERT)\\n\",\n",
    "    \"    embeddings = []\\n\",\n",
    "    \"    for sentence in sentences:\\n\",\n",
    "    \"        doc = nlp(sentence)\\n\",\n",
    "    \"        embeddings.append(doc.vector)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calcular similitud con todo el documento\\n\",\n",
    "    \"    doc_vector = nlp(text).vector\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    similarities = []\\n\",\n",
    "    \"    for embedding in embeddings:\\n\",\n",
    "    \"        # Calcular similitud coseno\\n\",\n",
    "    \"        similarity = sum(embedding * doc_vector) / (sum(embedding * embedding) ** 0.5 * sum(doc_vector * doc_vector) ** 0.5)\\n\",\n",
    "    \"        similarities.append(similarity)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Obtener índices de las oraciones más similares\\n\",\n",
    "    \"    top_indices = sorted(range(len(similarities)), \\n\",\n",
    "    \"                        key=lambda i: similarities[i], \\n\",\n",
    "    \"                        reverse=True)[:num_sentences]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Ordenar por posición original\\n\",\n",
    "    \"    top_indices = sorted(top_indices)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    summary = [sentences[i] for i in top_indices]\\n\",\n",
    "    \"    return summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Medir tiempo de ejecución y generar resúmenes\\n\",\n",
    "    \"bert_times = []\\n\",\n",
    "    \"bert_summaries = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"=== RESUMEN USANDO BERT (aproximación con SpaCy) ===\\\")\\n\",\n",
    "    \"for i, carta in enumerate(cartas_originals):\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    summary = bert_summarize(carta)\\n\",\n",
    "    \"    execution_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    bert_times.append(execution_time)\\n\",\n",
    "    \"    bert_summaries.append(summary)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nCarta {i+1} - Tiempo de ejecución: {execution_time:.3f} segundos\\\")\\n\",\n",
    "    \"    print(\\\"Resumen:\\\")\\n\",\n",
    "    \"    for j, sentence in enumerate(summary):\\n\",\n",
    "    \"        print(f\\\"{j+1}. {sentence[:100]}...\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"e8f2eb19-8326-49fd-8d2f-636b139e56be\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"#### 4.6 LSA (Latent Semantic Analysis)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"28a5d9eb-4ff8-438e-9b49-87fd2e1f4a91\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def lsa_summarize(text, num_sentences=4):\\n\",\n",
    "    \"    \\\"\\\"\\\"Genera resumen usando LSA\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Tokenizar oraciones\\n\",\n",
    "    \"    sentences = sent_tokenize(text)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Crear matriz de documentos-términos\\n\",\n",
    "    \"    vectorizer = CountVectorizer(stop_words='english')\\n\",\n",
    "    \"    document_matrix = vectorizer.fit_transform(sentences)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Aplicar"
   ],
   "id": "3f3f9c0f99506268"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:00:38.406185Z",
     "start_time": "2025-05-04T23:00:38.366745Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "494f72c6a45a07d0",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1855278807.py, line 609)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 609\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31m\"    \\n\"\"\"\u001B[39m\n              ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m incomplete input\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7088462a6f0919ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
